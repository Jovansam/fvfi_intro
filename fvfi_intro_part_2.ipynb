{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Value Function Iteration (Part 2)\n",
    "\n",
    "Quentin Batista, The University of Tokyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. [The Bellman Operator and the Value Function Iteration (VFI) algorithm](#The-Bellman-Operator)\n",
    "2. [The approximate Bellman Operator and the Fitted Value Function Iteration (FVFI) algorithm](#The-Approximate-Bellman-Operator)\n",
    "3. [Solving the cake eating problem with FVFI in Python](#Python-Implementation)\n",
    "4. [The Curse of Dimensionality](#The-Curse-of-Dimensionality)\n",
    "5. [Brief, high-level discussion of variants on the basic algorithm](#Variants-on-the-Basic-Algorithm)\n",
    "6. [A Simple Stochastic DP Example](#Stochastic-DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "### Some comments on programming language\n",
    "\n",
    "Many people have strong opinions about which language is better.\n",
    "\n",
    "**Advantages of Matlab:**\n",
    "- Currently mainstream in Economics (legacy code is often in Matlab)\n",
    "\n",
    "**Disadvantages of Matlab:**\n",
    "- Closed-source\n",
    "- Often slow\n",
    "- Design\n",
    "\n",
    "**Advantages of Python:**\n",
    "- Well-developed ecosystem\n",
    "- Extremely versatile\n",
    "- Object-orientated language\n",
    "\n",
    "**Disadvantages of Python:**\n",
    "- Getting different tools to work together can be difficult\n",
    "\n",
    "**Advantages of Julia:**\n",
    "- Built for scientific computation\n",
    "- Dynare developers are working on a Julia version\n",
    "\n",
    "**Disadvantages of Julia:**\n",
    "- Changing very quickly\n",
    "\n",
    "\n",
    "Some people argue that Python is too slow for computational Economics (e.g. [Aruoba and Fernandez-Villaverde](https://github.com/jstac/julia_python_comparison/blob/master/Update_March_23_2018.pdf))\n",
    "\n",
    "Answer: [No, Python is Not Too Slow for Computational Economics](https://notes.quantecon.org/submission/5bae5cb538674f000fd2c8e3) by John Stachurski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional vs non-functional requirements \n",
    "\n",
    "In software engineering a functional requirement specifies what the system should do/how it should look, etc.\n",
    "\n",
    "Non-functional requirements specify how the system should be. For example:\n",
    "\n",
    "- Extensibility\n",
    "- Reliability   \n",
    "- Accessibility  \n",
    "- Maintainability  \n",
    "- Testability  \n",
    "- Scalability\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiled vs. interpreted languages\n",
    "\n",
    "**Interpreted:** Python, Javascript, Matlab, ...  \n",
    "**Compiled:** C++, Fortran, Julia (just-in-time), ...\n",
    "\n",
    "Workflow with compiled languages:\n",
    "1. Write your program\n",
    "2. Use a **compiler** to translate your program into machine readable code\n",
    "3. Run your program\n",
    "\n",
    "Workflow with interpreted languages:\n",
    "1. Write your program\n",
    "2. Run your program\n",
    "\n",
    "With interpreted languages, each line is read and executed **at runtime**. The interpreter is the program that takes care of getting your computer to run your program.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "x = 1.\n",
    "y = 2.\n",
    "x + y\n",
    "```\n",
    "\n",
    "Roughly speaking, this provides a much more dynamic environment but execution is relatively slower.\n",
    "\n",
    "**Tradeoff:** Developer productivity vs. execution time\n",
    "\n",
    "**Note 1:** This distinction is relatively arbitrary because the interpreter might act just like a compiler\n",
    "\n",
    "**Note 2:** Python now has a just-in-time compiler called Numba for numerical computation. This tool comes with additional restrictions on what you can do. We make extensive use of this tool in QuantEcon lectures. \n",
    "\n",
    "The general idea is that in Python you want to figure out which parts of your code are slow and optimize only those parts: \"Premature optimization is the root of all evil\" -- Donald Knuth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python tools for scientific computation\n",
    "\n",
    "- [NumPy](https://numpy.org/devdocs/reference/index.html): Data structure for numerical computation + some linear algebra\n",
    "- [SciPy](https://docs.scipy.org/doc/scipy/reference/): Scientific computation from solving optimization problem, linear algebra, interpolation, solving differential equations, etc...\n",
    "- [Pandas](https://pandas.pydata.org/): Data structure + tools for data analysis\n",
    "- [Matplotlib](https://matplotlib.org/): Plotting library\n",
    "- [SymPy](https://www.sympy.org/en/index.html): Symbolic computation\n",
    "- [Numba](http://numba.pydata.org/): Makes your code fast\n",
    "- [Interpolation](https://github.com/EconForge/interpolation.py): Fast interpolation \n",
    "- [JAX](https://github.com/google/jax): Automatic differentiation + code optimization\n",
    "\n",
    "### Some Python tools for economic modelling\n",
    "\n",
    "- [QuantEcon.py](https://quanteconpy.readthedocs.io/en/latest/): Tools to support your own code (e.g. discrete DP, discretization of AR(1) process, LQ control problems, some optimizers, etc...).\n",
    "- [Dolo](https://dolo.readthedocs.io/en/latest/): More or less equivalent to `Dynare`.\n",
    "- [Econ-ARK](https://econ-ark.org/): Toolkit for heterogenuous agents structural modeling.\n",
    "\n",
    "If you **really** love neural networks: [DeepEquilibriumNets](https://github.com/sischei/DeepEquilibriumNets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install interpolation matplotlib quantecon numba==0.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from interpolation import interp\n",
    "from scipy.optimize import minimize_scalar, minimize, curve_fit\n",
    "from numba import njit, prange \n",
    "from quantecon.optimize import scalar_maximization\n",
    "from interpolation.splines import eval_linear, UCGrid\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Mathematics is the art of giving the same thing different names\" (Henri Poincaré)\n",
    "\n",
    "class CakeEating:\n",
    "    def __init__(self,\n",
    "                 β=0.96,           # discount factor\n",
    "                 γ=1.5,            # degree of relative risk aversion\n",
    "                 x_grid_min=1e-3,  # exclude zero for numerical stability\n",
    "                 x_grid_max=2.5,   # size of cake\n",
    "                 x_grid_size=120):\n",
    "\n",
    "        self.β, self.γ = β, γ\n",
    "\n",
    "        # Set up grid\n",
    "        self.x_grid = np.linspace(x_grid_min, x_grid_max, x_grid_size)\n",
    "\n",
    "    # Utility function\n",
    "    def u(self, c):\n",
    "\n",
    "        γ = self.γ\n",
    "\n",
    "        if γ == 1:\n",
    "            return np.log(c)\n",
    "        else:\n",
    "            return (c ** (1 - γ)) / (1 - γ)\n",
    "\n",
    "    # first derivative of utility function\n",
    "    def u_prime(self, c):\n",
    "\n",
    "        return c ** (-self.γ)\n",
    "\n",
    "    def state_action_value(self, c, x, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation given x and c.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        u, β = self.u, self.β\n",
    "        v = lambda x: interp(self.x_grid, v_array, x)\n",
    "\n",
    "        return u(c) + β * v(x - c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the `CakeEating` class\n",
    "# ce.β\n",
    "ce = CakeEating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to solve the maximization problem\n",
    "def maximize(g, a, b, args):\n",
    "    \"\"\"\n",
    "    Maximize the function g over the interval [a, b].\n",
    "\n",
    "    We use the fact that the maximizer of g on any interval is\n",
    "    also the minimizer of -g.  The tuple args collects any extra\n",
    "    arguments to g.\n",
    "\n",
    "    Returns the maximal value and the maximizer.\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum\n",
    "\n",
    "\n",
    "def T(v, ce):\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "    * ce is an instance of CakeEating\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "\n",
    "    for i, x in enumerate(ce.x_grid):\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        v_new[i] = maximize(ce.state_action_value, 1e-10, x, (x, v))[1]  # v_new[i] = v'(x_i)\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a plot showing how the value function changes at each iteration\n",
    "x_grid = ce.x_grid\n",
    "v = ce.u(x_grid)       # Initial guess\n",
    "n = 12                 # Number of iterations\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, v, color=plt.cm.jet(0),\n",
    "        lw=2, alpha=0.6, label='Initial guess')\n",
    "\n",
    "for i in range(n):\n",
    "    v = T(v, ce)  # Apply the Bellman operator\n",
    "    ax.plot(x_grid, v, color=plt.cm.jet(i / n), lw=2, alpha=0.6)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('value', fontsize=12)\n",
    "ax.set_xlabel('cake size $x$', fontsize=12)\n",
    "ax.set_title('Value function iterations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(ce,\n",
    "                           tol=1e-4,\n",
    "                           max_iter=1000,\n",
    "                           verbose=True,\n",
    "                           print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ce.x_grid)) # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:  # `max_iter` is used to ensure termination of algorithm\n",
    "        v_new = T(v, ce)\n",
    "\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "v = compute_value_function(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute analytical solution at grid points\n",
    "def v_star(x, β, γ):\n",
    "    return (1 - β**(1 / γ))** (-γ) * (x ** (1-γ) / (1-γ))\n",
    "\n",
    "\n",
    "v_analytical = v_star(ce.x_grid, ce.β, ce.γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a plot that compares the numerical and analytical solutions\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, v_analytical, label='analytical solution')\n",
    "ax.plot(x_grid, v, label='numerical solution')\n",
    "ax.set_ylabel('$V(x)$', fontsize=12)\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.legend()\n",
    "ax.set_title('Comparison between analytical and numerical value functions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's going on near 0?**\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "Lv^{*}\\left(x_{0}\\right)&\\approx&\\max_{0\\leq c\\leq x_{0}}u\\left(c\\right)+\\beta Lv^{*}\\left(x_{0}-c\\right)\\\\&=&\\max_{0\\leq c\\leq x_{0}}u\\left(c\\right)+\\beta Lv^{*}\\left(x_{0}\\right)\\\\&=&u\\left(x_{0}\\right)+\\beta u\\left(x_{0}\\right)+\\beta^{2}u\\left(x_{0}\\right)+\\dots\\\\&=&\\frac{u\\left(x_{0}\\right)}{1-\\beta}\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.u(x_grid.min()) / (1 - ce.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.u(x_grid.min() * 0.99) / (1 - ce.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(ce, v):\n",
    "    \"\"\"\n",
    "    The optimal policy function. Given the value function,\n",
    "    it finds optimal consumption in each state.\n",
    "\n",
    "    * ce is an instance of CakeEating\n",
    "    * v is a value function array\n",
    "\n",
    "    \"\"\"\n",
    "    c = np.empty_like(v)\n",
    "\n",
    "    for i in range(len(ce.x_grid)):\n",
    "        x = ce.x_grid[i]\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        c[i] = maximize(ce.state_action_value, 1e-10, x, (x, v))[0]\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "c = σ(ce, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_star(x, β, γ):\n",
    "\n",
    "    return (1 - β ** (1/γ)) * x\n",
    "\n",
    "\n",
    "c_analytical = c_star(ce.x_grid, ce.β, ce.γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ce.x_grid, c_analytical, label='analytical')\n",
    "ax.plot(ce.x_grid, c, label='Numerical')\n",
    "ax.set_ylabel(r'$\\sigma(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "We ask the following question: how does the number of points in our grid change when we increase the number of dimensions of the state vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_nb = 12\n",
    "d = np.arange(1, dims_nb)\n",
    "points_per_d = 10\n",
    "grid_points = points_per_d ** d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d, grid_points, 'o');\n",
    "plt.xlabel('number of dimensions')\n",
    "plt.ylabel('total numer of grid points')\n",
    "# 1e11 = one hundred billion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both memory requirements and the amount of computation explode exponentially as the number of dimensions grow!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many economic models feature high-dimensional state spaces:\n",
    "- International Real Business Cycle models\n",
    "- Dynamic Stochastic General Equilibrium (DSGE) models\n",
    "- Overlapping Generations (OLG) models\n",
    "- Mathematical finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU-bound\n",
    "\n",
    "\"In computer science, a computer is CPU-bound (or compute-bound) when the time for it to complete a task is determined principally by the speed of the central processor: processor utilization is high, perhaps at 100% usage for many seconds or minutes. \" (Wikipedia)\n",
    "\n",
    "### Memory-bound\n",
    "\n",
    "\"Memory bound refers to a situation in which the time to complete a given computational problem is decided primarily by the amount of memory required to hold data.\" (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_nb = 15\n",
    "\n",
    "try:\n",
    "    np.ones((points_per_d, ) * dims_nb)\n",
    "except MemoryError as e: \n",
    "    print('Error message:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants on the Basic Algorithm\n",
    "\n",
    "### Different Numerical Grids\n",
    "\n",
    "**Basic idea**: Choose points in a more thoughtful manner\n",
    "\n",
    "- `np.geomspace` vs. `np.linspace`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_grid = np.geomspace(ce.x_grid.min(), ce.x_grid.max(), num=ce.x_grid.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "idx = np.arange(0, ce.x_grid.size)\n",
    "plt.plot(idx, ce.x_grid, 'o', markersize=2., label='np.linspace')\n",
    "plt.plot(idx, geom_grid, 'o', markersize=2., label='geomspace')\n",
    "plt.xlabel('index', size=16)\n",
    "plt.ylabel(r'$x$', size=16)\n",
    "plt.legend()\n",
    "plt.title('np.linspace vs np.geomspace');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_geom = CakeEating()\n",
    "ce_geom.x_grid = geom_grid  # Modifies the grid inplacey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_geom = compute_value_function(ce_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(x_grid, v, label='np.linspace')\n",
    "ax.plot(x_grid, interp(geom_grid, v_geom, x_grid), label='np.geomspace')\n",
    "ax.set_ylabel('$V(x)$', fontsize=12)\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fancy: [Adaptive sparse grids](http://johannesbrumm.com/wp-content/uploads/2017/09/Brumm-Scheidegger-2017-ECTA.pdf)\n",
    "\n",
    "<img src=\"full_vs_sparse_grid.png\" width=\"400\">\n",
    "\n",
    "<img src=\"adaptive_sparse_test.png\"  width=\"1000\">\n",
    "\n",
    "*Source: [2]*\n",
    "\n",
    "**Important note:** There are grid-free methods for doing numerical DP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Approximation Methods\n",
    "\n",
    "Packages in Python: [interpolation.py](https://github.com/EconForge/interpolation.py), [SciPy](https://docs.scipy.org/doc/scipy/reference/interpolate.html)\n",
    "\n",
    "- Piecewise linear\n",
    "- Various types of polynomials ([Chebychev](https://en.wikipedia.org/wiki/Chebyshev_polynomials), [Hermite](https://en.wikipedia.org/wiki/Hermite_polynomials))\n",
    "- [Splines](https://en.wikipedia.org/wiki/Spline_(mathematics)) (i.e. piecewise polynomial)\n",
    "- Neural Networks (tools: [TensorFlow](https://github.com/tensorflow/tensorflow), [PyTorch](https://github.com/pytorch/pytorch), [Keras](https://keras.io/))\n",
    "\n",
    "On Neural Networks: \n",
    "- Very popular recently\n",
    "- [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) is \"smoke and mirrors\"\n",
    "\n",
    "#### Things to take into consideration:\n",
    "\n",
    "- Nonexpansive approximation (see [6])\n",
    "- Shape preservation (see [7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider the following functional form: $w_{i}\\left(y\\right)\\equiv w\\left(y;a_{i},b_{i}\\right)=a_{i}y^{b_{i}}$.\n",
    "\n",
    "(Recall the analytical solution is $v^{*}\\left(x\\right)=\\left(1-\\beta^{\\frac{1}{\\gamma}}\\right)^{-\\gamma}\\frac{x^{1-\\gamma}}{1-\\gamma}$) \n",
    "\n",
    "#### Procedure\n",
    "\n",
    "Start with $a_{0}$ and $b_{0}$. \n",
    "\n",
    "Compute $\\hat{T}v_{i+1}\\left(x\\right)=\\max_{c\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c\\right)+\\beta w_{i}\\left(x-c\\right)\\right\\} $ for all $x$.\n",
    "\n",
    "Compute $w_{i+1}$ by solving $\\min_{\\left(a_{i+1},b_{i+1}\\right)}N^{-1}\\sum_{j=1}^{N}\\left(w\\left(x_{j};a_{i+1},b_{i+1}\\right)-\\hat{T}v_{i+1}\\left(x_{j}\\right)\\right)^{2}$.  (We'll use `scipy.optimize.curve_fit` in Python)\n",
    "\n",
    "Iterate until convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w(y, a, b):  # New function\n",
    "    return a * y ** b\n",
    "\n",
    "\n",
    "def T(v, ce, a, b):  # Changed here\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "    * ce is an instance of CakeEating\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "\n",
    "    for i, x in enumerate(ce.x_grid):\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        objective = lambda c, a, b: ce.u(c) + ce.β * w(x - c, a, b)  # Changed here\n",
    "        v_new[i] = maximize(objective, 1e-10, x, (a, b))[1]\n",
    "        \n",
    "    a_new, b_new = curve_fit(w, x_grid, v_new)[0]  # Changed here\n",
    "\n",
    "    return v_new, a_new, b_new  # Changed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(ce,\n",
    "                           tol=1e-4,\n",
    "                           max_iter=1000,\n",
    "                           verbose=True,\n",
    "                           print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ce.x_grid)) # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "    a, b = 1, 1.  # New variables\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_new, a, b = T(v, ce, a, b)  # Changed here\n",
    "\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new, a, b  # Changed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_param, a, b = compute_value_function(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, v_analytical, label='analytical solution')\n",
    "ax.plot(x_grid, v_param, label='numerical solution')\n",
    "ax.set_ylabel('$V(x)$', fontsize=12)\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.legend()\n",
    "ax.set_title('Comparison between analytical and numerical value functions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Maximization Routines\n",
    "\n",
    "- [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)\n",
    "- [quantecon.optimize](https://quanteconpy.readthedocs.io/en/latest/optimize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to solve the maximization problem\n",
    "def maximize(g, x0, args):  # Changed here\n",
    "    \"\"\"\n",
    "    Maximize the function g over the interval [a, b].\n",
    "\n",
    "    We use the fact that the maximizer of g on any interval is\n",
    "    also the minimizer of -g.  The tuple args collects any extra\n",
    "    arguments to g.\n",
    "\n",
    "    Returns the maximal value and the maximizer.\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    # Previously: result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    result = minimize(objective, x0, bounds=[[1e-10, args[0]]], method='L-BFGS-B')  # Changed here\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum\n",
    "\n",
    "\n",
    "def T(v, ce):\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "    * ce is an instance of CakeEating\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "\n",
    "    for i, x in enumerate(ce.x_grid):\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        x0 = x / 10\n",
    "        v_new[i] = maximize(ce.state_action_value, x0, (x, v))[1]  # Changed here\n",
    "\n",
    "    return v_new\n",
    "\n",
    "\n",
    "def compute_value_function(ce,\n",
    "                           tol=1e-4,\n",
    "                           max_iter=1000,\n",
    "                           verbose=True,\n",
    "                           print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ce.x_grid)) # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_new = T(v, ce)\n",
    "\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "v_diff_optimizer = compute_value_function(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, v, label='original optimizer')\n",
    "ax.plot(x_grid, v_diff_optimizer, label='new optimizer')\n",
    "#ax.plot(x_grid, v_analytical, label='analytical')\n",
    "ax.set_ylabel('$V(x)$', fontsize=12)\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.legend()\n",
    "ax.set_title('Comparison between analytical and numerical value functions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_diff_optimizer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce.u(x_grid.min()) / (1 - ce.β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "\n",
    "\"The approximate $|S|$ of\n",
    "chess, shogi, and Go are $10^{47}$, $10^{71}$, and $10^{171}$, respectively, which are comparable to the\n",
    "number of atoms in the observable universe ($10^{78}$ ∼ $10^{82}$) and certainly larger than the total\n",
    "information-storage capacity of humanity (in the order of $10^{20}$ bytes).\" (Igami, 2018)\n",
    "\n",
    "https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "\n",
    "<img src=\"fig_8_sutton_barto.png\">\n",
    "\n",
    "*Source: [4]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exhaustive Search Direction\n",
    "\n",
    "Expand the Bellman equation:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "v\\left(x\\right)&=&\\max_{c\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c\\right)+\\beta v\\left(x-c\\right)\\right\\} \\\\&=&\\max_{c_{1}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{1}\\right)+\\beta\\max_{c_{2}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{2}\\right)+\\beta v\\left(x-c_{1}-c_{2}\\right)\\right\\} \\right\\} \\\\&=&\\max_{c_{1}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{1}\\right)+\\beta\\max_{c_{2}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{2}\\right)+\\beta\\max_{c_{3}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{3}\\right)+\\beta v\\left(x-c_{1}-c_{2}-c_{3}\\right)\\right\\} \\right\\} \\right\\} \n",
    "\\end{eqnarray}$$\n",
    "\n",
    "You can repeat this as many times as you want.\n",
    "\n",
    "Let $T'v\\left(x\\right)=\\max_{c_{1}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{1}\\right)+\\beta\\max_{c_{2}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{2}\\right)+\\beta\\max_{c_{3}\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c_{3}\\right)+\\beta v\\left(x-c_{1}-c_{2}-c_{3}\\right)\\right\\} \\right\\} \\right\\} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CakeEating:\n",
    "    def __init__(self,\n",
    "                 β=0.96,           # discount factor\n",
    "                 γ=1.5,            # degree of relative risk aversion\n",
    "                 x_grid_min=1e-3,  # exclude zero for numerical stability\n",
    "                 x_grid_max=2.5,   # size of cake\n",
    "                 x_grid_size=120):\n",
    "\n",
    "        self.β, self.γ = β, γ\n",
    "\n",
    "        # Set up grid\n",
    "        self.x_grid = np.linspace(x_grid_min, x_grid_max, x_grid_size)\n",
    "\n",
    "    # Utility function\n",
    "    def u(self, c):\n",
    "\n",
    "        γ = self.γ\n",
    "\n",
    "        if γ == 1:\n",
    "            return np.log(c)\n",
    "        else:\n",
    "            return (c ** (1 - γ)) / (1 - γ)\n",
    "\n",
    "    # first derivative of utility function\n",
    "    def u_prime(self, c):\n",
    "\n",
    "        return c ** (-self.γ)\n",
    "\n",
    "    def state_action_value(self, c, x, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation given x and c.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        u, β = self.u, self.β\n",
    "        v = lambda x: interp(self.x_grid, v_array, x)\n",
    "\n",
    "        return u(c[0]) + β * u(c[1]) + β ** 2 * v(x - c[0] - c[1])  # Changed here\n",
    "    \n",
    "\n",
    "# Define a function to solve the maximization problem\n",
    "def maximize(g, x0, args):  # Changed here\n",
    "    \"\"\"\n",
    "    Maximize the function g over the interval [a, b].\n",
    "\n",
    "    We use the fact that the maximizer of g on any interval is\n",
    "    also the minimizer of -g.  The tuple args collects any extra\n",
    "    arguments to g.\n",
    "\n",
    "    Returns the maximal value and the maximizer.\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    # Previously: result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    result = minimize(objective, x0, bounds=[[1e-10, args[0]], [1e-10, args[0]]], method='L-BFGS-B')  # Changed here\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum\n",
    "\n",
    "\n",
    "def T(v, ce):\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "    * ce is an instance of CakeEating\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "\n",
    "    for i, x in enumerate(ce.x_grid):\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        x0 = np.array([x / 10, x / 10])\n",
    "        v_new[i] = maximize(ce.state_action_value, x0, (x, v))[1]  # Changed here\n",
    "\n",
    "    return v_new\n",
    "\n",
    "\n",
    "def compute_value_function(ce,\n",
    "                           tol=1e-4,\n",
    "                           max_iter=1000,\n",
    "                           verbose=True,\n",
    "                           print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ce.x_grid)) # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_new = T(v, ce)\n",
    "\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ce = CakeEating()\n",
    "v_new = compute_value_function(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-off:** Each iteration is \"closer to reality\" but more expensive to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, v, label='Iterate on $T$')\n",
    "ax.plot(x_grid, v_new, label=r\"Iterate on $T'$\")\n",
    "ax.set_ylabel('$V(x)$', fontsize=12)\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.legend()\n",
    "ax.set_title('Comparison between different methods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computing and High Performance Computing\n",
    "\n",
    "**Basic idea:** Use multiple processors at the same time\n",
    "\n",
    "Most likely, your computer uses a Central Processing Unit (CPU) to run computations. \n",
    "\n",
    "**What's a CPU?**\n",
    "\n",
    "\"A central processing unit (CPU), also called a central processor or main processor, is the electronic circuitry within a computer that executes instructions that make up a computer program.The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program.\" (Wikipedia)\n",
    "\n",
    "A CPU core is a CPU's processor. Modern CPUs usually have multiple cores.\n",
    "\n",
    "A thread is a sequence of instructions within a program that can be executed independently of other code. Modern CPU cores generally allow for having multiple threads. This is called hyper-threading. \n",
    "\n",
    "My laptop has 2 cores and 4 threads. Modern, consumer-grade CPUs have up to 64 cores and 128 threads (see [here](https://www.cpu-monkey.com/en/cpu-amd_ryzen_threadripper_3990x-977)). \n",
    "\n",
    "**Serial vs. parallel computing**\n",
    "\n",
    "<img src=\"parallel_1.png\" width='600'>\n",
    "<img src=\"parallel_2.png\" width='600'>\n",
    "\n",
    "First, stack the objects that need to be computed at a given iteration:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "Tv_{i+1}\\left(x_{1}\\right)&=&\\max_{c\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c\\right)+\\beta v_{i}\\left(x_{1}-c\\right)\\right\\} \\\\Tv_{i+1}\\left(x_{2}\\right)&=&\\max_{c\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c\\right)+\\beta v_{i}\\left(x_{2}-c\\right)\\right\\} \\\\\\vdots&\\vdots&\\vdots\\\\Tv_{i+1}\\left(x_{N}\\right)&=&\\max_{c\\in\\Gamma\\left(x\\right)}\\left\\{ u\\left(c\\right)+\\beta v_{i}\\left(x_{N}-c\\right)\\right\\} \n",
    "\\end{eqnarray}$$\n",
    "\n",
    "$Tv_{i+1}\\left(x_{0}\\right)$ and $Tv_{i+1}\\left(x_{1}\\right) $ can be computed **independently** so why not do them in parallel?\n",
    "\n",
    "#### Is this hard?\n",
    "\n",
    "It depends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrote the code to work with Numba\n",
    "def bellman_operator_factory(x_grid, γ, β):\n",
    "    if γ == 1:\n",
    "        @njit\n",
    "        def u(c):\n",
    "            return np.log(c)    \n",
    "    else:\n",
    "        @njit\n",
    "        def u(c):\n",
    "            return (c ** (1 - γ)) / (1 - γ)\n",
    "    \n",
    "    @njit\n",
    "    def state_action_value(c, x, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation given x and c.\n",
    "        \"\"\"\n",
    "        return u(c) + β * interp(x_grid, v_array, x - c)\n",
    "    \n",
    "    \n",
    "    @njit(parallel=True)  # Equivalent to T = njit(T, parallel=True)\n",
    "    def T(v):\n",
    "        \"\"\"\n",
    "        The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "        * ce is an instance of CakeEating\n",
    "        * v is an array representing a guess of the value function\n",
    "\n",
    "        \"\"\"\n",
    "        v_new = np.empty_like(v)\n",
    "\n",
    "        for i in prange(len(x_grid)):\n",
    "            # Maximize RHS of Bellman equation at state x\n",
    "            v_new[i] = scalar_maximization.brent_max(state_action_value, 1e-10, x_grid[i], (x_grid[i], v))[1]\n",
    "\n",
    "        return v_new\n",
    "    \n",
    "    return T\n",
    "\n",
    "\n",
    "T = bellman_operator_factory(ce.x_grid, ce.γ, ce.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(ce,\n",
    "                           tol=1e-4,\n",
    "                           max_iter=1000,\n",
    "                           verbose=True,\n",
    "                           print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ce.x_grid)) # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "    T = bellman_operator_factory(ce.x_grid, ce.γ, ce.β)\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_new = T(v)\n",
    "\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ce = CakeEating(x_grid_size=100_000)\n",
    "v_parallel = compute_value_function(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_grid, v, label='base')\n",
    "ax.plot(x_grid, interp(ce.x_grid, v_parallel, x_grid), label=r\"parallel\")\n",
    "ax.set_ylabel('$V(x)$', fontsize=12)\n",
    "ax.set_xlabel('$x$', fontsize=12)\n",
    "ax.legend()\n",
    "ax.set_title('Comparison between different methods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More about parallel computing and HPC**\n",
    "\n",
    "- [Simon Scheidegger](https://sites.google.com/site/simonscheidegger/home)\n",
    "- https://github.com/sischei/OSE2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic DP\n",
    "\n",
    "Consider the following modification of the cake eating problem:\n",
    "\n",
    "$$\\max_{\\left\\{ c_{t}\\right\\} _{t=0}^{\\infty}}\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\beta^{t}u\\left(c_{t}\\right)\\right]$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$x_{t+1}=\\left(1-\\delta_{t+1}\\right)\\left(x_{t}-c_{t}\\right)$$\n",
    "\n",
    "where $\\mathbb{P}\\left(\\delta_{t}=0\\right)=\\mathbb{P}\\left(\\delta_{t}=0.05\\right)=0.5$. (i.e. the agent loses 5% of his remaining cake at each period with probability $\\frac{1}{2}$).\n",
    "\n",
    "The associated Bellman equation is:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "v^{*}\\left(x\\right)&=&\\sup_{0\\leq c\\leq x}u\\left(c\\right)+\\beta\\mathbb{E}\\left[v^{*}\\left(x'\\right)\\right]\\\\&=&\\sup_{0\\leq c\\leq x}u\\left(c\\right)+\\beta\\sum_{\\delta'}\\mathbb{\\mathbb{P}}\\left(\\delta'\\right)v^{*}\\left(x'\\right)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "The associated Bellman operator satisfies:\n",
    "\n",
    "$$\\left(Tv\\right)\\left(x\\right)=\\sup_{0\\leq c\\leq x}u\\left(c\\right)+\\beta\\sum_{\\delta'}\\mathbb{\\mathbb{P}}\\left(\\delta'\\right)v\\left(x'\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CakeEating:\n",
    "    def __init__(self,\n",
    "                 β=0.96,           # discount factor\n",
    "                 γ=1.5,            # degree of relative risk aversion\n",
    "                 π=0.5,            # P(δ=0)  -- changed here\n",
    "                 x_grid_min=1e-3,  # exclude zero for numerical stability\n",
    "                 x_grid_max=2.5,   # size of cake\n",
    "                 x_grid_size=120):\n",
    "\n",
    "        self.β, self.γ = β, γ\n",
    "        self.δ_vals = np.array([0., 0.05])  # Changed here\n",
    "        self.π = π  # Changed here\n",
    "\n",
    "        # Set up grid\n",
    "        self.x_grid = np.linspace(x_grid_min, x_grid_max, x_grid_size)\n",
    "\n",
    "    # Utility function\n",
    "    def u(self, c):\n",
    "\n",
    "        γ = self.γ\n",
    "\n",
    "        if γ == 1:\n",
    "            return np.log(c)\n",
    "        else:\n",
    "            return (c ** (1 - γ)) / (1 - γ)\n",
    "\n",
    "    # first derivative of utility function\n",
    "    def u_prime(self, c):\n",
    "\n",
    "        return c ** (-self.γ)\n",
    "\n",
    "    def state_action_value(self, c, x, v_array):  # Changed this function\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation given x and c.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        u, β = self.u, self.β\n",
    "        δ_vals = self.δ_vals\n",
    "        π = self.π\n",
    "        \n",
    "        v = lambda x: interp(self.x_grid, v_array, x)\n",
    "        Ev = π * v((1 - δ_vals[0]) * (x - c)) + (1 - π) * v((1 - δ_vals[1]) * (x - c))\n",
    "\n",
    "        return u(c) + β * Ev\n",
    "    \n",
    "\n",
    "# Define a function to solve the maximization problem\n",
    "def maximize(g, a, b, args):\n",
    "    \"\"\"\n",
    "    Maximize the function g over the interval [a, b].\n",
    "\n",
    "    We use the fact that the maximizer of g on any interval is\n",
    "    also the minimizer of -g.  The tuple args collects any extra\n",
    "    arguments to g.\n",
    "\n",
    "    Returns the maximal value and the maximizer.\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum\n",
    "\n",
    "\n",
    "def T(v, ce):\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "    * ce is an instance of CakeEating\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "\n",
    "    for i, x in enumerate(ce.x_grid):\n",
    "        # Maximize RHS of Bellman equation at state x\n",
    "        v_new[i] = maximize(ce.state_action_value, 1e-10, x, (x, v))[1]  # v_new[i] = v'(x_i)\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(ce,\n",
    "                           tol=1e-4,\n",
    "                           max_iter=1000,\n",
    "                           verbose=True,\n",
    "                           print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ce.x_grid)) # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:  # `max_iter` is used to ensure termination of algorithm\n",
    "        v_new = T(v, ce)\n",
    "\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = CakeEating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_stoch = compute_value_function(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_grid, v, label='original problem')\n",
    "plt.plot(x_grid, v_stoch, label='modified problem')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "**Basic idea:** Sample the expectation instead of computing it exactly\n",
    "\n",
    "Let: \n",
    "\n",
    "$$Q\\left(x,c\\right)=u\\left(c\\right)+\\beta\\mathbb{E}\\left[v^{*}\\left(x'\\right)\\right]$$\n",
    "\n",
    "so that:\n",
    "\n",
    "$$v^{*}\\left(x\\right)=\\sup_{0\\leq c\\leq x}Q\\left(x,c\\right)$$.\n",
    "\n",
    "We'll focus on computing $Q\\left(x,c\\right)$ instead of $v^{*}\\left(x\\right)$.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "First, generate a sample $x_{t+1}$ given $c_{t}$ and $x_{t}$ \n",
    "\n",
    "Then, compute:\n",
    "\n",
    "$$\\underbrace{Q^{new}\\left(x_{t},c_{t}\\right)}_{\\mathrm{new\\,value}}\\leftarrow\\underbrace{Q\\left(x_{t},c_{t}\\right)}_{\\mathrm{old\\,value}}+\\underbrace{\\alpha}_{\\mathrm{learning\\,rate}}\\underbrace{\\left(u\\left(c_{t}\\right)+\\beta\\max_{c}Q\\left(x_{t+1},c\\right)-Q\\left(x_{t},c_{t}\\right)\\right)}_{\\mathrm{temporal\\,difference}}$$\n",
    "\n",
    "When $\\alpha=1$, notice that this becomes:\n",
    "\n",
    "$$Q^{new}\\left(x_{t},c_{t}\\right)\\leftarrow u\\left(c_{t}\\right)+\\beta\\max_{c}Q\\left(x_{t+1},c\\right)$$\n",
    "\n",
    "Notice that the update is stochastic. \n",
    "\n",
    "To get it to converge, at each iteration do:\n",
    "\n",
    "$$\\alpha\\leftarrow0.999\\alpha$$ \n",
    "\n",
    "(Note: This is a little bit different than the algorithm in [4] but captures the essential idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_operator_factory(x_grid, c_grid, γ, β, δ_vals):\n",
    "    x_grid_min = x_grid.min()\n",
    "    grid = UCGrid((x_grid.min(), x_grid.max(), x_grid.size), (c_grid.min(), c_grid.max(), c_grid.size))\n",
    "    eval_points = np.empty((c_grid.size, 2))\n",
    "    eval_points[:, 1] = c_grid\n",
    "    \n",
    "    if γ == 1:\n",
    "        @njit\n",
    "        def u(c):\n",
    "            return np.log(c)    \n",
    "    else:\n",
    "        @njit\n",
    "        def u(c):\n",
    "            return (c ** (1 - γ)) / (1 - γ)\n",
    "    \n",
    "    @njit\n",
    "    def T(Q, v, α):\n",
    "        Q_new = Q.copy()\n",
    "\n",
    "        for x_i, x in enumerate(x_grid):\n",
    "            for c_i, c in enumerate(c_grid):\n",
    "                δ_i = np.random.randint(2)\n",
    "                xp = (1 - δ_vals[δ_i]) * (x - c)\n",
    "                v_xp = interp(x_grid, v, xp)\n",
    "                \n",
    "                if xp > 0.:  # Only update well-defined state action pairs\n",
    "                    temp_diff = u(c) + β * v_xp - Q[x_i, c_i]\n",
    "                    Q_new[x_i, c_i] = Q[x_i, c_i] + α * (temp_diff)\n",
    "\n",
    "        return Q_new\n",
    "    \n",
    "    return T\n",
    "\n",
    "# Multiply min x by 0.99 for the (0, 0) state action pair to be well defined\n",
    "c_grid = np.linspace(x_grid.min() * 0.99, x_grid.max(), x_grid.size * 4)  \n",
    "T = bellman_operator_factory(ce.x_grid, c_grid, ce.γ, ce.β, ce.δ_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_itr = 50_000\n",
    "\n",
    "Δs = []\n",
    "Q = np.ones((ce.x_grid.size, c_grid.size)) * -100_000  # Trick to ignore state action pairs that are not well defined\n",
    "\n",
    "itr = 1\n",
    "α = 1.\n",
    "\n",
    "while True:\n",
    "    v = Q.max(axis=1)\n",
    "    Q_new = T(Q, v, α)\n",
    "    Δs.append(np.max(np.abs(Q_new - Q)))\n",
    "    Q = Q_new\n",
    "    \n",
    "    if itr % 1000 == 0:\n",
    "        print(f\"Error at iteration {itr} is {Δs[itr-1]}.\")\n",
    "    \n",
    "    if Δs[itr-1] < 1e-7:\n",
    "        break\n",
    "        \n",
    "    if itr == max_itr:\n",
    "        break\n",
    "    \n",
    "    itr += 1\n",
    "    α = α * 0.999  # Decay learning rate\n",
    "    \n",
    "    \n",
    "if itr == max_itr:\n",
    "    print(\"Failed to converge!\")\n",
    "\n",
    "if itr < max_itr:\n",
    "    print(f\"\\nConverged in {itr} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_itr = 100\n",
    "max_itr = 300\n",
    "\n",
    "plt.plot(np.arange(min_itr, max_itr+1), Δs[min_itr-1:max_itr])\n",
    "plt.ylabel(r'$\\left\\Vert Q^{new}-Q\\right\\Vert _{\\infty}$')\n",
    "plt.xlabel('Iteration');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(v_stoch, label='DP')\n",
    "plt.plot(Q.max(axis=1), label='Q-learning')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] John Stachurski, 2009. \"Economic Dynamics: Theory and Computation,\" MIT Press Books  \n",
    "[2] Johannes Brumm & Simon Scheidegger, 2017. \"Using Adaptive Sparse Grids to Solve High‐Dimensional Dynamic Models,\" Econometrica  \n",
    "[3] Artificial Intelligence as Structural Estimation: Economic Interpretations of Deep Blue, Bonanza, and AlphaGo, Mitsuru Igami  \n",
    "[4] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA.  \n",
    "[5] [Daisuke Oyama's](http://www.oyama.e.u-tokyo.ac.jp/) excellent lecture notes on Dynamic Programming  \n",
    "[6] John Stachurski, 2008. \"Continuous State Dynamic Programming via Nonexpansive Approximation,\" Computational Economics  \n",
    "[7] Yongyang Cai & Kenneth Judd, 2013. \"Shape-preserving dynamic programming,\" Mathematical Methods of Operations Research  \n",
    "[8] Cai, Yongyang & Judd, Kenneth. (2014). Advances in Numerical Dynamic Programming and New Applications. Handbook of Computational Economics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
